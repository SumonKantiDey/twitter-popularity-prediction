{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"twitter-popularity.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOWRlx2WCPt0cnhvQu+XeGm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"8b027628881a4da0b831aaaa8b340766":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_cee321025fdf4bdd8ee7a29beab23e4d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_573a35d72f0d4630b5ec1f79be7a130b","IPY_MODEL_4ea2be83e5dc4b5b86bdddd7fd957e80"]}},"cee321025fdf4bdd8ee7a29beab23e4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"573a35d72f0d4630b5ec1f79be7a130b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e641991a267947b89a2e55d57a62a577","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":6,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":6,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_15acc3e3086146a695db61a813d0e140"}},"4ea2be83e5dc4b5b86bdddd7fd957e80":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0b92ba0373a94a09862473c1914fdea4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 6/6 [00:03&lt;00:00,  1.89it/s, loss=1.45e+12]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_be784e6224f048be97c7073ec9503819"}},"e641991a267947b89a2e55d57a62a577":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"15acc3e3086146a695db61a813d0e140":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0b92ba0373a94a09862473c1914fdea4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"be784e6224f048be97c7073ec9503819":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Il1FKsDxXMq","executionInfo":{"status":"ok","timestamp":1628057835016,"user_tz":-360,"elapsed":7661,"user":{"displayName":"Sumon Kanti Dey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeKy4i8ax1KOVpgZGOlH_p_tgZ27lB8lIJ5KTcoA=s64","userId":"02609177644539095397"}},"outputId":"66b1bd6b-7c00-41a0-e19e-4a54b5b79a04"},"source":["!pip install transformers==3.3.0"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting transformers==3.3.0\n","  Downloading transformers-3.3.0-py3-none-any.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 4.3 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.0) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.0) (21.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.0) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.0) (3.0.12)\n","Collecting sentencepiece!=0.1.92\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 34.6 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 54.5 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.0) (4.41.1)\n","Collecting tokenizers==0.8.1.rc2\n","  Downloading tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n","\u001b[K     |████████████████████████████████| 3.0 MB 18.6 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.0) (1.19.5)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.3.0) (2.4.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.0) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.0) (3.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.0) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.0) (1.15.0)\n","Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n","Successfully installed sacremoses-0.0.45 sentencepiece-0.1.96 tokenizers-0.8.1rc2 transformers-3.3.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SA2glaRdri0d","executionInfo":{"status":"ok","timestamp":1627974685087,"user_tz":-360,"elapsed":602,"user":{"displayName":"Sumon Kanti Dey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeKy4i8ax1KOVpgZGOlH_p_tgZ27lB8lIJ5KTcoA=s64","userId":"02609177644539095397"}},"outputId":"52ea2b11-eeed-4776-af02-129b8432a22d"},"source":["import sys\n","import os\n","import json\n","from google.colab import drive\n","import pandas as pd\n","import numpy as np\n","import re\n","import requests\n","from tqdm.notebook import tqdm\n","import torch\n","import warnings\n","import transformers\n","import torch.nn as nn\n","import time\n","from transformers import BertTokenizer, BertModel\n","from sklearn import model_selection\n","from transformers import AdamW\n","from transformers import get_linear_schedule_with_warmup\n","\n","tqdm.pandas()\n","import warnings\n","import nltk, string\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","nltk.download('punkt') # if necessary...\n","pd.set_option('display.max_colwidth', 255)\n","warnings.filterwarnings(\"ignore\")\n","nltk.download('stopwords')\n","drive.mount('/content/drive')"],"execution_count":20,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rqTPGtpE1jrl","executionInfo":{"status":"ok","timestamp":1627974686481,"user_tz":-360,"elapsed":1,"user":{"displayName":"Sumon Kanti Dey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeKy4i8ax1KOVpgZGOlH_p_tgZ27lB8lIJ5KTcoA=s64","userId":"02609177644539095397"}}},"source":["def label_scaling(val):\n","    val = np.log(val + 1)\n","    scaler = MinMaxScaler()\n","    scaler.fit(val)\n","    val = scaler.transform(val)\n","    return scaler, val\n","\n","\n","def label_inverse_scaling(scaler, val):\n","    val = scaler.inverse_transform(val)\n","    val = np.exp(val) - 1\n","    return val\n","\n","\n","def seed_everything(seed=42):\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(seed)\n","    torch.cuda.manual_seed(seed)\n","\n","class AverageMeter:\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202},"id":"4_RLV52MsNve","executionInfo":{"status":"ok","timestamp":1627974688430,"user_tz":-360,"elapsed":466,"user":{"displayName":"Sumon Kanti Dey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeKy4i8ax1KOVpgZGOlH_p_tgZ27lB8lIJ5KTcoA=s64","userId":"02609177644539095397"}},"outputId":"b0eead76-f254-470a-a1ec-2a24738e4f79"},"source":["path = '/content/drive/MyDrive/twitter-popularity-prediction/'\n","df = pd.read_csv(f\"{path}data.csv\")\n","df.head(1)"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>user_id</th>\n","      <th>user_name</th>\n","      <th>location</th>\n","      <th>description</th>\n","      <th>follower_count</th>\n","      <th>friends_count</th>\n","      <th>verified</th>\n","      <th>tweet_id</th>\n","      <th>created_at</th>\n","      <th>num_of_likes</th>\n","      <th>retweet_count</th>\n","      <th>text</th>\n","      <th>user_location</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>813286</td>\n","      <td>Barack Obama</td>\n","      <td>Washington, DC</td>\n","      <td>Dad, husband, President, citizen.</td>\n","      <td>129803017</td>\n","      <td>590251</td>\n","      <td>True</td>\n","      <td>896523232098078720</td>\n","      <td>Sun Aug 13 00:06:09 +0000 2017</td>\n","      <td>4232344</td>\n","      <td>1515265</td>\n","      <td>\"No one is born hating another person because of the color of his skin or his background or his religion...\" https://t.co/InZ58zkoAm</td>\n","      <td>Washington, DC</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   user_id  ...   user_location\n","0   813286  ...  Washington, DC\n","\n","[1 rows x 13 columns]"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tfJkGdm34SLe","executionInfo":{"status":"ok","timestamp":1627975151169,"user_tz":-360,"elapsed":353,"user":{"displayName":"Sumon Kanti Dey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeKy4i8ax1KOVpgZGOlH_p_tgZ27lB8lIJ5KTcoA=s64","userId":"02609177644539095397"}},"outputId":"7fb1722f-ad66-4714-aee6-b8dbd305cf19"},"source":["df.num_of_likes.values"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([4232344, 3875332, 2790177, ...,  241649,  241647,  239607])"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"e5_QcwtntkPZ","executionInfo":{"status":"ok","timestamp":1627974692462,"user_tz":-360,"elapsed":568,"user":{"displayName":"Sumon Kanti Dey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeKy4i8ax1KOVpgZGOlH_p_tgZ27lB8lIJ5KTcoA=s64","userId":"02609177644539095397"}}},"source":["pretrained_model_name = '/content/drive/MyDrive/pre_trained_model/bert-base-uncased/'#'bert-base-uncased'\n","do_lower_case = True\n","max_len = 128\n","bert_hidden = 768\n","dropout = 0.3\n","train_batch_size=16\n","valid_batch_size=32\n","epochs = 1\n","learning_rate = 1e-3\n","adam_epsilon=1e-8\n","warmup_steps=0.01\n","seed_everything()"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TutUkMNqsvtw","executionInfo":{"status":"ok","timestamp":1627976812967,"user_tz":-360,"elapsed":336,"user":{"displayName":"Sumon Kanti Dey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeKy4i8ax1KOVpgZGOlH_p_tgZ27lB8lIJ5KTcoA=s64","userId":"02609177644539095397"}},"outputId":"b37e043a-e9aa-43b6-e5ef-23cf196a0b44"},"source":["class TweetDataset:\n","    def __init__(self, tweet, targets):\n","        self.tweet = tweet\n","        self.tokenizer = transformers.BertTokenizer.from_pretrained(pretrained_model_name,do_lower_case = do_lower_case)\n","        self.max_length = max_len\n","        self.targets = targets\n","\n","    def __len__(self):\n","        return len(self.tweet)\n","\n","    def __getitem__(self, item):\n","        \n","        tweet = str(self.tweet[item])\n","        tweet = \" \".join(tweet.split())\n","\n","        inputs = self.tokenizer.encode_plus(\n","            tweet,\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","            truncation_strategy=\"longest_first\",\n","            pad_to_max_length=True,\n","            truncation=True\n","        )\n","        \n","        ids = inputs[\"input_ids\"]\n","        mask = inputs[\"attention_mask\"]\n","        token_type_ids = inputs[\"token_type_ids\"]\n","        \n","        return {\n","            'ids': torch.tensor(ids, dtype=torch.long),\n","            'mask': torch.tensor(mask, dtype=torch.long),\n","            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n","            'targets': torch.tensor(self.targets[item], dtype=torch.float)\n","        }\n","\n","\n","df = pd.read_csv(f\"{path}data.csv\").dropna().reset_index(drop = True)\n","dset = TweetDataset(\n","        tweet=df.text.values,\n","        targets=df.num_of_likes.values\n","        )\n","print(df.iloc[0])\n","print(dset[10])"],"execution_count":35,"outputs":[{"output_type":"stream","text":["user_id                                                                                                                                         813286\n","user_name                                                                                                                                 Barack Obama\n","location                                                                                                                                Washington, DC\n","description                                                                                                          Dad, husband, President, citizen.\n","follower_count                                                                                                                               129803017\n","friends_count                                                                                                                                   590251\n","verified                                                                                                                                          True\n","tweet_id                                                                                                                            896523232098078720\n","created_at                                                                                                              Sun Aug 13 00:06:09 +0000 2017\n","num_of_likes                                                                                                                                   4232344\n","retweet_count                                                                                                                                  1515265\n","text              \"No one is born hating another person because of the color of his skin or his background or his religion...\" https://t.co/InZ58zkoAm\n","user_location                                                                                                                           Washington, DC\n","Name: 0, dtype: object\n","{'ids': tensor([  101,  7632,  7955,   999,  2067,  2000,  1996,  2434,  5047,  1012,\n","         2003,  2023,  2518,  2145,  2006,  1029,  9393,  1998,  1045,  2024,\n","         2125,  2006,  1037,  4248, 10885,  1010,  2059,  2057,  1521,  2222,\n","         2131,  2067,  2000,  2147,  1012,   102,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0]), 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0]), 'targets': tensor(1679188.)}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KqVbmF29wFWz","executionInfo":{"status":"ok","timestamp":1627976814313,"user_tz":-360,"elapsed":2,"user":{"displayName":"Sumon Kanti Dey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeKy4i8ax1KOVpgZGOlH_p_tgZ27lB8lIJ5KTcoA=s64","userId":"02609177644539095397"}}},"source":["class BertBaseUncased(nn.Module) :\n","    def __init__(self) : \n","      super(BertBaseUncased,self).__init__() \n","      self.bert = transformers.BertModel.from_pretrained('/content/drive/MyDrive/pre_trained_model/bert-base-uncased/', output_hidden_states=True) \n","      self.drop_out = nn.Dropout(dropout) \n","      self.l0 =  nn.Linear(bert_hidden * 2, 1)\n","      torch.nn.init.normal_(self.l0.weight, std=0.02)\n","\n","    def forward(self,ids,attention_mask,token_type_ids):\n","      out = self.bert(\n","          ids,\n","          attention_mask=attention_mask,\n","          token_type_ids=token_type_ids\n","      )\n","      print(\"out = \",out[-1])\n","      out = torch.cat((out[-1], out[-2]), dim=-1)\n","      #out = self.drop_out(out)\n","      out = out[:,0,:]\n","      logits = self.l0(out)\n","      return logits\n","\n","class BertBaseUncasedNext(nn.Module) :\n","    def __init__(self) : \n","      super(BertBaseUncasedNext,self).__init__() \n","      self.bert = transformers.BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True) \n","      self.drop_out = nn.Dropout(0.1) \n","      self.l0 =  nn.Linear(768 * 2, 1)\n","      torch.nn.init.normal_(self.l0.weight, std=0.02)\n","        \n","    def _get_cls_vec(self, vec):\n","      return vec[:,0,:].view(-1, 768)\n","    def forward(self,ids,attention_mask,token_type_ids):\n","      _, _, hidden_states = self.bert(\n","          ids,\n","          attention_mask=attention_mask,\n","          token_type_ids=token_type_ids\n","      )\n","      vec1 = self._get_cls_vec(hidden_states[-1])\n","      vec2 = self._get_cls_vec(hidden_states[-2])\n","\n","      out = torch.cat([vec1, vec2], dim=1)\n","      #out = self.drop_out(out)\n","      logits = self.l0(out)\n","      return logits\n"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"YMxyMBkBx3dr","executionInfo":{"status":"ok","timestamp":1627979997926,"user_tz":-360,"elapsed":347,"user":{"displayName":"Sumon Kanti Dey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeKy4i8ax1KOVpgZGOlH_p_tgZ27lB8lIJ5KTcoA=s64","userId":"02609177644539095397"}}},"source":["def loss_fn(y_pred, y_true):\n","  #loss_func = nn.L1Loss(reduction='mean') #MAE\n","  loss_func = nn.MSELoss(reduction='mean')\n","  return loss_func(y_pred, y_true.view(-1,1))\n","\n","def train_fn(data_loader, model, optimizer, device, scheduler, n_examples):\n","  model.train()\n","  losses = AverageMeter()\n","  tk0 = tqdm(data_loader, total=len(data_loader))\n","  start = time.time()\n","  train_losses = []\n","  fin_targets = []\n","  fin_outputs = []\n","  for bi, d in enumerate(tk0):\n","      ids = d[\"ids\"]\n","      mask = d[\"mask\"]\n","      token_type_ids = d[\"token_type_ids\"]\n","      targets = d[\"targets\"]\n","      ids = ids.to(device, dtype=torch.long)\n","      mask = mask.to(device, dtype=torch.long)\n","      token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","      targets = targets.to(device, dtype=torch.float)\n","      # Reset gradients\n","      model.zero_grad()\n","\n","      outputs = model(\n","          ids=ids,\n","          attention_mask=mask,\n","          token_type_ids = token_type_ids \n","      )\n","\n","      loss = loss_fn(outputs, targets)\n","      train_losses.append(loss.item())\n","\n","      outputs = torch.round(nn.ReLU()(outputs)).squeeze()\n","      print(\"outputs = \", outputs)\n","      targets = targets.squeeze()\n","      print(\"targets = \", targets)\n","      outputs = outputs.cpu().detach().numpy().tolist()\n","      targets = targets.cpu().detach().numpy().tolist()\n","\n","      end = time.time()\n","      if (bi % 2 == 0 and bi != 0) or (bi == len(data_loader) - 1):\n","        print(f'bi={bi},Train loss={loss.item()}, time={end-start}')\n","      \n","      loss.backward() # Calculate gradients based on loss\n","      nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","      optimizer.step() # Adjust weights based on calculated gradients\n","      scheduler.step() # Update scheduler\n","      losses.update(loss.item(), ids.size(0))\n","      tk0.set_postfix(loss = losses.avg)\n","      fin_targets.extend(targets) \n","      fin_outputs.extend(outputs)\n","\n","  return np.mean(train_losses)\n","\n","def eval_fn(data_loader, model, device, n_examples):\n","  model.eval()\n","  start = time.time()\n","  losses = AverageMeter()\n","  val_losses = []\n","  fin_targets = []\n","  fin_outputs = []\n","  with torch.no_grad():\n","      #tk0 = tqdm(data_loader, total=len(data_loader))\n","      for bi, d in enumerate(data_loader):\n","          ids = d[\"ids\"]\n","          mask = d[\"mask\"]\n","          token_type_ids = d[\"token_type_ids\"]\n","          targets = d[\"targets\"]\n","          ids = ids.to(device, dtype=torch.long)\n","          mask = mask.to(device, dtype=torch.long)\n","          token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","          targets = targets.to(device, dtype=torch.float)\n","\n","          outputs = model(\n","              ids=ids,\n","              attention_mask=mask,\n","              token_type_ids = token_type_ids \n","          )\n","          loss = loss_fn(outputs, targets)\n","          val_losses.append(loss.item())\n","\n","          targets = targets.squeeze()\n","          outputs = torch.round(nn.ReLU()(outputs)).squeeze()\n","          if isinstance(targets.cpu().detach().numpy().tolist(), list) == False:\n","              fin_targets.append(targets.cpu().detach().numpy().tolist())\n","              fin_outputs.append(outputs.cpu().detach().numpy().tolist())\n","          else:\n","              fin_targets.extend(targets.cpu().detach().numpy().tolist())\n","              fin_outputs.extend(outputs.cpu().detach().numpy().tolist())\n","  return fin_outputs,fin_targets, np.mean(val_losses)"],"execution_count":47,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":729,"referenced_widgets":["8b027628881a4da0b831aaaa8b340766","cee321025fdf4bdd8ee7a29beab23e4d","573a35d72f0d4630b5ec1f79be7a130b","4ea2be83e5dc4b5b86bdddd7fd957e80","e641991a267947b89a2e55d57a62a577","15acc3e3086146a695db61a813d0e140","0b92ba0373a94a09862473c1914fdea4","be784e6224f048be97c7073ec9503819"]},"id":"jGK5EukguDkP","executionInfo":{"status":"ok","timestamp":1627980005288,"user_tz":-360,"elapsed":6172,"user":{"displayName":"Sumon Kanti Dey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeKy4i8ax1KOVpgZGOlH_p_tgZ27lB8lIJ5KTcoA=s64","userId":"02609177644539095397"}},"outputId":"1cbdd99f-f565-4a8c-f5cd-22ea143b72d4"},"source":["import gc\n","from sklearn.metrics import mean_squared_log_error\n","\n","def run():\n","  dfx = pd.read_csv(f\"{path}data.csv\").dropna().reset_index(drop=True)\n","  dfx = dfx[:100]\n","  df_train, df_valid = model_selection.train_test_split(\n","      dfx, \n","      test_size=0.15, \n","      random_state=46, \n","    )\n","\n","  print(\"train len - {} valid len - {}\".format(len(df_train), len(df_valid)))\n","  df_train = df_train.reset_index(drop=True)\n","  df_valid = df_valid.reset_index(drop=True)\n","\n","  df_train = df_train.sample(frac=1).reset_index(drop=True)\n","\n","  train_dataset = TweetDataset(\n","      tweet=df_train.text.values,\n","      targets=df_train.num_of_likes.values\n","  )\n","\n","  train_data_loader = torch.utils.data.DataLoader(\n","      train_dataset,\n","      batch_size=train_batch_size,\n","      shuffle=True,\n","      num_workers=4\n","  )\n","\n","  valid_dataset = TweetDataset(\n","      tweet=df_valid.text.values,\n","      targets=df_valid.num_of_likes.values\n","  )\n","\n","  valid_data_loader = torch.utils.data.DataLoader(\n","      valid_dataset,\n","      batch_size=valid_batch_size,\n","      num_workers=2\n","  )\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","  model = BertBaseUncasedNext()#BertBaseUncased()\n","  model.to(device)\n","  \n","\n","  param_optimizer = list(model.named_parameters())\n","  no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","  # Define two sets of parameters: those with weight decay, and those without\n","  optimizer_parameters = [\n","      {\n","          \"params\": [\n","              p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n","          ],\n","          \"weight_decay\": 0.001,\n","      },\n","      {\n","          \"params\": [\n","              p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n","          ],\n","          \"weight_decay\": 0.0,\n","      },\n","  ]\n","\n","  num_train_steps = int(len(df_train) / train_batch_size * epochs)\n","\n","  optimizer = AdamW(optimizer_parameters, lr=learning_rate, eps=adam_epsilon)\n","  '''\n","  Create a scheduler to set the learning rate at each training step\n","  \"Create a schedule with a learning rate that decreases linearly after linearly increasing during a warmup period.\" (https://pytorch.org/docs/stable/optim.html)\n","  Since num_warmup_steps = 0, the learning rate starts at 3e-5, and then linearly decreases at each training step\n","  '''\n","  scheduler = get_linear_schedule_with_warmup(\n","      optimizer,\n","      num_warmup_steps=warmup_steps,\n","      num_training_steps=num_train_steps\n","  )\n","  #es = utils.EarlyStopping(patience=15, mode=\"max\")\n","  print(\"STARTING TRAINING for ...\\n\")\n","  #history = defaultdict(list)\n","  loss_history_epoch = []\n","  metric_history_epoch = []\n","\n","  best_accuracy = 999.9\n","\n","  for epoch in range(epochs):\n","      print(f'Epoch {epoch + 1}/{epochs}')\n","      print('-' * 10)\n","      \n","\n","      train_loss = train_fn(train_data_loader, model, optimizer, device, scheduler, len(df_train))\n","      print(f'Train metric {train_loss}')\n","\n","      fin_pred, fin_val, val_loss = eval_fn(valid_data_loader, model, device, len(df_valid))\n","\n","      val_metric = mean_squared_log_error(fin_val, fin_pred)\n","\n","      print(f'Val loss {val_loss} Val metric {val_metric}')\n","      if val_metric < best_accuracy:\n","        best_accuracy = val_metric\n","        #torch.save(model.state_dict(), f\"{args.model_path}{args.model_specification}.bin\")\n","\n","  \n","  del model, optimizer, scheduler, train_data_loader, valid_data_loader, train_dataset, valid_dataset\n","  torch.cuda.empty_cache()\n","  torch.cuda.synchronize()\n","  print(\"##################################### Task End ############################################\")\n","  print(gc.collect())\n","\n","run()"],"execution_count":48,"outputs":[{"output_type":"stream","text":["train len - 85 valid len - 15\n","STARTING TRAINING for ...\n","\n","Epoch 1/1\n","----------\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8b027628881a4da0b831aaaa8b340766","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["outputs =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       device='cuda:0', grad_fn=<SqueezeBackward0>)\n","targets =  tensor([ 654288.,  526896.,  712295.,  728360.,  497719., 1016263.,  535184.,\n","         637321.,  479570.,  628259.,  667864.,  506084., 1498500., 1339922.,\n","         461050., 1860006.], device='cuda:0')\n","outputs =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       device='cuda:0', grad_fn=<SqueezeBackward0>)\n","targets =  tensor([ 667129.,  548434., 1903138., 1378404., 1342905., 1818139., 1324427.,\n","         610281.,  928345.,  927893., 3875332., 1882187.,  622518., 1523207.,\n","         829631.,  699043.], device='cuda:0')\n","outputs =  tensor([6., 6., 6., 6., 6., 6., 6., 6., 6., 4., 6., 6., 6., 6., 6., 5.],\n","       device='cuda:0', grad_fn=<SqueezeBackward0>)\n","targets =  tensor([ 546957.,  495856.,  517509.,  996676.,  609899., 1229216., 4232344.,\n","         495958.,  754652., 1769516.,  493081.,  539133.,  546773.,  725104.,\n","         585508.,  807262.], device='cuda:0')\n","bi=2,Train loss=1743837986816.0, time=1.2198166847229004\n","outputs =  tensor([13., 12., 12., 13., 12., 12., 13., 13., 12., 12., 12., 12., 12., 12.,\n","        12., 12.], device='cuda:0', grad_fn=<SqueezeBackward0>)\n","targets =  tensor([1049306.,  502477.,  773899.,  550596.,  644114.,  528882., 1030574.,\n","        1311369.,  638119., 1229321., 2790177.,  752152., 1442268., 2648442.,\n","         617645.,  508045.], device='cuda:0')\n","outputs =  tensor([15., 15., 16., 15., 16., 15., 16., 15., 15., 15., 15., 15., 15., 15.,\n","        15., 15.], device='cuda:0', grad_fn=<SqueezeBackward0>)\n","targets =  tensor([ 787279.,  707373.,  460407.,  463968.,  888224.,  836656.,  552989.,\n","         737764., 1304058., 1288929.,  572360.,  839268.,  649209.,  555899.,\n","        1804028., 1447358.], device='cuda:0')\n","bi=4,Train loss=897484849152.0, time=2.06791090965271\n","outputs =  tensor([17., 17., 17., 17., 17.], device='cuda:0', grad_fn=<SqueezeBackward0>)\n","targets =  tensor([1028304.,  507101.,  552630., 1458571., 1197817.], device='cuda:0')\n","bi=5,Train loss=1036399083520.0, time=2.378556251525879\n","\n","Train metric 1406318553770.6667\n","Val loss 885055225856.0 Val metric 115.02481164184088\n","##################################### Task End ############################################\n","2276\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MK6WVF7DmTjl"},"source":["\n","\n","def main():\n","\n","    t = Timer()\n","    with t.timer(f'fix seed RANDOM_STATE:{RANDOM_STATE}'):\n","        seed_everything(RANDOM_STATE)\n","\n","    with t.timer(f'read label'):\n","        y_train = pd.read_csv(f'{INPUT_DIR}/train.solution', header=None).T.values[0].reshape(-1, 1)\n","\n","    if LABEL_LOG_SCALING is True:\n","        with t.timer(f'label log scaling (log->mms[0, 1]'):\n","            scaler, y_train = label_scaling(y_train)\n","\n","    with t.timer(f'read features'):\n","        unique_num_dic = {}\n","        feature_index = {}\n","\n","        X_train = pd.DataFrame()\n","        X_valid = pd.DataFrame()\n","        X_test = pd.DataFrame()\n","        fidx = 0\n","        for feat in dense_features:\n","            logging.info(f'[dense][{feat}] read feature ...')\n","            feature_index[feat] = fidx\n","            fidx += 1\n","            X_train = pd.concat([\n","                X_train, pd.read_feather(f'{FEATURE_DIR}/{feat}_train.feather')\n","            ], axis=1)\n","            X_valid = pd.concat([\n","                X_valid, pd.read_feather(f'{FEATURE_DIR}/{feat}_valid.feather')\n","            ], axis=1)\n","            X_test = pd.concat([\n","                X_test, pd.read_feather(f'{FEATURE_DIR}/{feat}_test.feather')\n","            ], axis=1)\n","        for feat in sparse_features:\n","            logging.info(f'[sparse][{feat}] read feature ...')\n","            feature_index[feat] = fidx\n","            fidx += 1\n","            X_train = pd.concat([\n","                X_train, pd.read_feather(f'{FEATURE_DIR}/{feat}_train.feather')\n","            ], axis=1)\n","            X_valid = pd.concat([\n","                X_valid, pd.read_feather(f'{FEATURE_DIR}/{feat}_valid.feather')\n","            ], axis=1)\n","            X_test = pd.concat([\n","                X_test, pd.read_feather(f'{FEATURE_DIR}/{feat}_test.feather')\n","            ], axis=1)\n","            unique_num = pd.concat([\n","                X_train[feat], X_valid[feat], X_test[feat]\n","            ]).nunique()\n","            unique_num_dic[feat] = unique_num\n","        for feat in varlen_sparse_features:\n","            logging.info(f'[varlen sparse][{feat}] read feature ...')\n","            feature_index[feat] = (fidx, fidx + VARLEN_MAX_LEN)\n","            fidx += VARLEN_MAX_LEN\n","\n","            train_feat = pd.read_feather(f'{FEATURE_DIR}/{feat}_train.feather').values\n","            varlen_list = [i[0] for i in train_feat]\n","            varlen_list = pad_sequences(varlen_list, maxlen=VARLEN_MAX_LEN, padding='post', )\n","            X_train = pd.concat([\n","                X_train, pd.DataFrame(varlen_list)\n","            ], axis=1)\n","\n","            valid_feat = pd.read_feather(f'{FEATURE_DIR}/{feat}_valid.feather').values\n","            varlen_list = [i[0] for i in valid_feat]\n","            varlen_list = pad_sequences(varlen_list, maxlen=VARLEN_MAX_LEN, padding='post', )\n","            X_valid = pd.concat([\n","                X_valid, pd.DataFrame(varlen_list)\n","            ], axis=1)\n","\n","            test_feat = pd.read_feather(f'{FEATURE_DIR}/{feat}_test.feather').values\n","            varlen_list = [i[0] for i in test_feat]\n","            varlen_list = pad_sequences(varlen_list, maxlen=VARLEN_MAX_LEN, padding='post', )\n","            X_test = pd.concat([\n","                X_test, pd.DataFrame(varlen_list)\n","            ], axis=1)\n","\n","            tmp = []\n","            for i in [i[0] for i in train_feat] + [i[0] for i in valid_feat] + [i[0] for i in test_feat]:\n","                tmp.extend(i)\n","            unique_num = len(set(tmp))\n","            unique_num_dic[feat] = unique_num\n","        X_train = X_train.fillna(0.0)\n","        X_valid = X_valid.fillna(0.0)\n","        X_test = X_test.fillna(0.0)\n","\n","    logging.info('SPARSE FEATURE UNIQUE_NUM')\n","    print(unique_num_dic)\n","\n","    with t.timer(f'READ folds'):\n","        folds = pd.read_csv(f'{FOLD_DIR}/train_folds_{FOLD_NAME}{FOLD_NUM}_RS{RANDOM_STATE}.csv')\n","\n","    mlflow.set_experiment(EXP_NAME)\n","    mlflow.start_run()\n","    run_id = mlflow.active_run().info.run_id\n","\n","    fold_best_scores = {}  # fold_idx:best_cv_score\n","    for fold_idx in range(FOLD_NUM):\n","\n","        trn_idx = folds[folds.kfold != fold_idx].index.tolist()\n","        val_idx = folds[folds.kfold == fold_idx].index.tolist()\n","\n","        x_trn = X_train.iloc[trn_idx]\n","        y_trn = y_train[trn_idx]\n","        x_val = X_train.iloc[val_idx]\n","        y_val = y_train[val_idx]\n","\n","        train_loader = SimpleDataLoader(\n","            [torch.from_numpy(x_trn.values), torch.from_numpy(y_trn)],\n","            batch_size=BATCH_SIZE,\n","            shuffle=True\n","        )\n","\n","        model = load_model(\n","            feature_index=feature_index,\n","            unique_num_dic=unique_num_dic,\n","        )\n","        loss_func = set_loss(loss_name=LOSS)\n","        optim = torch.optim.Adam(model.parameters(), lr=LR)\n","        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=EPOCH_NUM)\n","\n","        loss_history = []\n","\n","        steps_per_epoch = (len(x_trn) - 1) // BATCH_SIZE + 1\n","        best_score = 999.9\n","        for epoch in range(EPOCH_NUM):\n","\n","            loss_history_epoch = []\n","            metric_history_epoch = []\n","\n","            logging.info(f'[{DEVICE}][FOLD:{fold_idx}] EPOCH - {epoch} / {EPOCH_NUM}')\n","            model = model.train()\n","            for bi, (bx, by) in tqdm(enumerate(train_loader), total=steps_per_epoch):\n","\n","                optim.zero_grad()\n","\n","                bx = bx.to(DEVICE).float()\n","                by = by.to(DEVICE).float().squeeze()\n","                y_pred = model(bx).squeeze()\n","\n","                loss = 0.0\n","                for loss_f in loss_func:\n","                    loss += loss_f(y_pred, by)\n","                loss = loss + model.reg_loss.item()\n","\n","                loss.backward()\n","                optim.step()\n","\n","                y_pred_np = y_pred.cpu().detach().numpy().reshape(-1, 1)\n","                y_np = by.cpu().detach().numpy().reshape(-1, 1)\n","\n","                try:\n","                    if LABEL_LOG_SCALING is True:\n","                        y_pred_inv = label_inverse_scaling(scaler, y_pred_np)\n","                        y_inv = label_inverse_scaling(scaler, y_np)\n","                        mlse = mean_squared_log_error(y_inv, y_pred_inv)\n","                    else:\n","                        mlse = mean_squared_log_error(y_np, y_pred_np)\n","                    loss_history_epoch.append(loss.item())\n","                    metric_history_epoch.append(mlse)\n","                except:\n","                    continue\n","\n","            scheduler.step()\n","            trn_loss_epoch = sum(loss_history_epoch) / len(loss_history_epoch)\n","            trn_metric_epoch = sum(metric_history_epoch) / len(metric_history_epoch)\n","\n","            preds_val = model.predict(x_val, BATCH_SIZE)\n","            val_loss = 0.0\n","            for loss_f in loss_func:\n","                val_loss += loss_f(torch.from_numpy(preds_val.reshape(-1, 1)), torch.from_numpy(y_val)).item()\n","\n","            try:\n","                if LABEL_LOG_SCALING is True:\n","                    preds_val_inv = label_inverse_scaling(scaler, preds_val.reshape(-1, 1))\n","                    y_val_inv = label_inverse_scaling(scaler, y_val)\n","                    val_metric = mean_squared_log_error(y_val_inv, preds_val_inv)\n","                else:\n","                    val_metric = mean_squared_log_error(y_val, preds_val)\n","            except:\n","                continue\n","\n","            logging.info(f'Train - Loss: {trn_loss_epoch}, MSLE: {trn_metric_epoch}')\n","            logging.info(f'Valid - Loss: {val_loss}, MSLE: {val_metric}')\n","            loss_history.append([\n","                epoch, trn_loss_epoch, trn_metric_epoch, val_loss, val_metric\n","            ])\n","\n","            if val_metric < best_score:\n","                best_score = val_metric\n","                weight_path = f'{SAVE_DIR}/model/train_weights_mlflow-{run_id}_fold{fold_idx}.h5'\n","                torch.save(model.state_dict(), weight_path)\n","                fold_best_scores[fold_idx] = (best_score, weight_path)\n","                mlflow.log_artifact(weight_path)\n","\n","        history_path = f'{SAVE_DIR}/model/loss_history-{run_id}_fold{fold_idx}.csv'\n","        pd.DataFrame(loss_history, columns=['epoch', 'trn_loss', 'trn_metric', 'val_loss', 'val_metric']).to_csv(history_path)\n","        mlflow.log_artifact(history_path)\n","\n","    cv = 0.0\n","    for fold_idx in range(FOLD_NUM):\n","        cv += fold_best_scores[fold_idx][0]\n","    cv /= FOLD_NUM\n","\n","    preds_train_val = np.zeros(len(X_train))\n","    for fold_idx in range(FOLD_NUM):\n","\n","        val_idx = folds[folds.kfold == fold_idx].index.tolist()\n","        x_val = X_train.iloc[val_idx]\n","\n","        model = load_model(\n","            feature_index=feature_index,\n","            unique_num_dic=unique_num_dic,\n","        )\n","        weight_path = fold_best_scores[fold_idx][1]\n","        model.load_state_dict(torch.load(weight_path))\n","\n","        preds_train_val_fold = model.predict(x_val, BATCH_SIZE)\n","        preds_train_val[val_idx] = preds_train_val_fold\n","\n","        preds_valid = model.predict(X_valid, BATCH_SIZE)\n","        X_valid[f'preds_{fold_idx}'] = preds_valid\n","\n","        preds_test = model.predict(X_test, BATCH_SIZE)\n","        X_test[f'preds_{fold_idx}'] = preds_test\n","\n","    X_train['preds'] = preds_train_val\n","    X_valid['preds'] = X_valid[[f'preds_{fold_idx}' for fold_idx in range(FOLD_NUM)]].mean()\n","    X_test['preds'] = X_test[[f'preds_{fold_idx}' for fold_idx in range(FOLD_NUM)]].mean()\n","\n","    save_path = f'{SAVE_DIR}/predict/preds_train_val_{run_id}.csv'\n","    X_train['preds'].to_csv(save_path, index=False, header=None)\n","    mlflow.log_artifact(save_path)\n","\n","    save_path = f'{SAVE_DIR}/predict/preds_valid_{run_id}.csv'\n","    X_valid['preds'].to_csv(save_path, index=False, header=None)\n","    mlflow.log_artifact(save_path)\n","\n","    save_path = f'{SAVE_DIR}/predict/preds_test_{run_id}.csv'\n","    X_test['preds'].to_csv(save_path, index=False, header=None)\n","    mlflow.log_artifact(save_path)\n","\n","    save_mlflow(run_id, cv, fold_best_scores)\n","    mlflow.end_run()\n","\n","\n","if __name__ == \"__main__\":\n","\n","    main()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f98Zs-r0p_H4","executionInfo":{"status":"ok","timestamp":1627975061125,"user_tz":-360,"elapsed":338,"user":{"displayName":"Sumon Kanti Dey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeKy4i8ax1KOVpgZGOlH_p_tgZ27lB8lIJ5KTcoA=s64","userId":"02609177644539095397"}},"outputId":"8b138aa4-fc6c-46ed-8456-fc1b1aedfc8d"},"source":["# example of a normalization\n","from numpy import asarray\n","from sklearn.preprocessing import MinMaxScaler\n","# define data\n","data = asarray([[112, 0.001],\n","\t\t\t\t[8, 0.05],\n","\t\t\t\t[50, 0.005],\n","\t\t\t\t[88, 0.07],\n","\t\t\t\t[4, 0.1]])\n","print(data)\n","# define min max scaler\n","scaler = MinMaxScaler()\n","# transform data\n","scaled = scaler.fit_transform(data)\n","print(scaled)\n","\n","scaler = MinMaxScaler()\n","scaler.fit(data)\n","val = scaler.transform(data)\n","print(val)\n","\n","def label_inverse_scaling(scaler, val):\n","    val = scaler.inverse_transform(val)\n","    val = np.exp(val) - 1\n","    return val\n"],"execution_count":27,"outputs":[{"output_type":"stream","text":["[[1.12e+02 1.00e-03]\n"," [8.00e+00 5.00e-02]\n"," [5.00e+01 5.00e-03]\n"," [8.80e+01 7.00e-02]\n"," [4.00e+00 1.00e-01]]\n","[[1.         0.        ]\n"," [0.03703704 0.49494949]\n"," [0.42592593 0.04040404]\n"," [0.77777778 0.6969697 ]\n"," [0.         1.        ]]\n","[[1.         0.        ]\n"," [0.03703704 0.49494949]\n"," [0.42592593 0.04040404]\n"," [0.77777778 0.6969697 ]\n"," [0.         1.        ]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Vx58Gk5Bwexw","executionInfo":{"status":"ok","timestamp":1627975062313,"user_tz":-360,"elapsed":3,"user":{"displayName":"Sumon Kanti Dey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeKy4i8ax1KOVpgZGOlH_p_tgZ27lB8lIJ5KTcoA=s64","userId":"02609177644539095397"}}},"source":["import numpy as np \n","def label_scaling(val):\n","    val = np.log(val + 1)\n","    scaler = MinMaxScaler()\n","    scaler.fit(val)\n","    val = scaler.transform(val)\n","    return scaler, val\n","\n","\n","def label_inverse_scaling(scaler, val):\n","    val = scaler.inverse_transform(val)\n","    val = np.exp(val) - 1\n","    return val\n","\n","scaler, val = label_scaling(data)"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h0IzqYktyPhc","executionInfo":{"status":"ok","timestamp":1627975064889,"user_tz":-360,"elapsed":5,"user":{"displayName":"Sumon Kanti Dey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeKy4i8ax1KOVpgZGOlH_p_tgZ27lB8lIJ5KTcoA=s64","userId":"02609177644539095397"}},"outputId":"71a68023-d794-4416-8ca7-0172b8cfdba1"},"source":["scaler"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MinMaxScaler(copy=True, feature_range=(0, 1))"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ge8I_wWjyTbT","executionInfo":{"status":"ok","timestamp":1627975066572,"user_tz":-360,"elapsed":3,"user":{"displayName":"Sumon Kanti Dey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeKy4i8ax1KOVpgZGOlH_p_tgZ27lB8lIJ5KTcoA=s64","userId":"02609177644539095397"}},"outputId":"02efb440-183f-40e4-da56-0433e47559da"},"source":["val"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1.        , 0.        ],\n","       [0.18851703, 0.5067365 ],\n","       [0.74484446, 0.04228621],\n","       [0.92342678, 0.70680382],\n","       [0.        , 1.        ]])"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CC9eOOQJyb7M","executionInfo":{"status":"ok","timestamp":1627975069357,"user_tz":-360,"elapsed":333,"user":{"displayName":"Sumon Kanti Dey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeKy4i8ax1KOVpgZGOlH_p_tgZ27lB8lIJ5KTcoA=s64","userId":"02609177644539095397"}},"outputId":"a7e116ef-1545-42fa-9860-1a686bf5c19e"},"source":["da = label_inverse_scaling(scaler,val)\n","da"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1.12e+02, 1.00e-03],\n","       [8.00e+00, 5.00e-02],\n","       [5.00e+01, 5.00e-03],\n","       [8.80e+01, 7.00e-02],\n","       [4.00e+00, 1.00e-01]])"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x8MYLuO3ytBl","executionInfo":{"status":"ok","timestamp":1627976467585,"user_tz":-360,"elapsed":368,"user":{"displayName":"Sumon Kanti Dey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeKy4i8ax1KOVpgZGOlH_p_tgZ27lB8lIJ5KTcoA=s64","userId":"02609177644539095397"}},"outputId":"bc694bfb-234a-4007-84c1-c192e60bd4c1"},"source":["def set_loss(loss_name):\n","    if type(loss_name) == str:\n","        if loss_name == 'MSE':\n","            loss_func = nn.MSELoss(reduction='mean')\n","        elif loss_name == 'MAE':\n","            loss_func = nn.L1Loss(reduction='mean')\n","        elif loss_name == 'Huber':\n","            loss_func = nn.SmoothL1Loss(reduction='mean')\n","        elif loss_name == 'LogCosh':\n","            loss_func = LogCoshLoss()\n","        loss_func_list = [loss_func]\n","    elif type(loss_name) == list:\n","        loss_func_list = []\n","        for ln in loss_name:\n","            if ln == 'MSE':\n","                loss_func = nn.MSELoss(reduction='mean')\n","            elif ln == 'MAE':\n","                loss_func = nn.L1Loss(reduction='mean')\n","            elif ln == 'Huber':\n","                loss_func = nn.SmoothL1Loss(reduction='mean')\n","            elif ln == 'LogCosh':\n","                loss_func = LogCoshLoss()\n","            loss_func_list.append(loss_func)\n","    return loss_func_list\n","set_loss(loss_name='MSE')"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[MSELoss()]"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"Y5W0Tqr8_dmV"},"source":[""],"execution_count":null,"outputs":[]}]}